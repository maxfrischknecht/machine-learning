{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download dataset from Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/maxfrischknecht/.cache/kagglehub/datasets/tarundalal/anime-quotes/versions/1\n"
     ]
    }
   ],
   "source": [
    "# This gets saved locally on your machine, check the path for the location\n",
    "path = kagglehub.dataset_download(\"tarundalal/anime-quotes\")\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset with a given filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully:\n",
      "                                               Quote         Character  \\\n",
      "0  People’s lives don’t end when they die, it end...     Itachi Uchiha   \n",
      "1  If you don’t take risks, you can’t create a fu...    Monkey D Luffy   \n",
      "2   If you don’t like your destiny, don’t accept it.    Naruto Uzumaki   \n",
      "3       When you give up, that’s when the game ends.  Mitsuyoshi Anzai   \n",
      "4  All we can do is live until the day we die. Co...      Deneil Young   \n",
      "\n",
      "                             Anime  \n",
      "0                           Naruto  \n",
      "1                        One Piece  \n",
      "2                           Naruto  \n",
      "3                        Slam Dunk  \n",
      "4  Uchuu Kyoudai or Space Brothers  \n"
     ]
    }
   ],
   "source": [
    "filename = \"AnimeQuotes.csv\"  # Specify the filename here\n",
    "dataset_path = os.path.join(path, filename)\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(\"Dataset loaded successfully:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    raise FileNotFoundError(f\"File {filename} not found in the dataset directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract quotes and preprocess text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"Quote\" column from the DataFrame\n",
    "quotes = df['Quote']\n",
    "\n",
    "# Convert all values in the column to strings (in case they are not already)\n",
    "quotes = quotes.astype(str)\n",
    "\n",
    "# Convert the column into a Python list (so it can be processed further)\n",
    "quotes = quotes.tolist()\n",
    "\n",
    "# quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenize the text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[306, 307, 18, 101, 28, 32, 61, 5, 141, 28, 32, 55, 308]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tokenizer object to process the text\n",
    "# This object will be used to convert words into numerical values (tokens).\n",
    "tokenizer = Tokenizer() # num_words=5000 limit to improve model based on low learning performance\n",
    "\n",
    "# Fit the tokenizer on the list of quotes\n",
    "# This builds a vocabulary based on the words in the quotes dataset\n",
    "# Counts the frequency of each word in the dataset.\n",
    "# Assigns a unique index to each word.\n",
    "tokenizer.fit_on_texts(quotes)\n",
    "\n",
    "# Converts each quote into a sequence of numbers based on the learned word indices.\n",
    "sequences = tokenizer.texts_to_sequences(quotes)\n",
    "sequences[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create input-output pairs for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, especially in sequence models, we train a model using input-output pairs where:\n",
    "- The input is a partial sequence of words.\n",
    "- The output is the next word that the model should predict.\n",
    "- This code creates input-output pairs for training the text generation model using n-gram-like sequences.\n",
    "- It progressively increases the sequence length so that the model learns contextual relationships between words.\n",
    "- It helps the model predict the next word in a sequence.\n",
    "\n",
    "\n",
    "```py\n",
    "[10, 15]            # \"The power\"\n",
    "[10, 15, 22]        # \"The power is\"\n",
    "[10, 15, 22, 8]     # \"The power is strong\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store input sequences\n",
    "input_sequences = []\n",
    "\n",
    "# Loop through each tokenized sequence (each quote in numerical format)\n",
    "for seq in sequences:\n",
    "    # Generate progressively growing sub-sequences from each full sequence\n",
    "    for i in range(1, len(seq)):  \n",
    "        # Append a sub-sequence that starts from the first word up to the i-th word\n",
    "        input_sequences.append(seq[:i+1])\n",
    "\n",
    "# input_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pad sequences and define input & output\n",
    "\n",
    "- Loops through all input_sequences and finds the longest sequence. (deep learning models, require fixed-size inputs.)\n",
    "- Some sequences are shorter, so we pad them with zeros to match the longest sequence. (Ensures alignment for LSTMs, which process sequences from left to right. The actual words stay at the end, and LSTMs focus on recent words.)\n",
    "- We then split them into `x_train_input` which are the word sequences minus the last word (which get saved into `y_train_input`) to learn to predict the next word\n",
    "\n",
    "| **X (Input Sequence)**  | **y (Label - Next Word)** |\n",
    "|-------------------------|--------------------------|\n",
    "| `[10]`                 | `15` (\"power\")           |\n",
    "| `[10, 15]`             | `22` (\"is\")             |\n",
    "| `[10, 15, 22]`         | `8` (\"strong\")          |\n",
    "\n",
    "- Convert y labels (word indices) into *one-hot encoded vectors*. This is necessary because the output layer of the model expects a probability distribution\n",
    "\n",
    "```py\n",
    "# from\n",
    "y = [3, 7, 1, 9]\n",
    "# to (if there would be 10 possible words)\n",
    "[\n",
    "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],  # Word ID 3\n",
    "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],  # Word ID 7\n",
    "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # Word ID 1\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   # Word ID 9\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the longest sequence length in input_sequences\n",
    "# This ensures all sequences will be padded to the same maximum length\n",
    "max_sequence_length = max([len(seq) for seq in input_sequences]) # Increase length,  Increase the sequence length so the model sees more context.\n",
    "\n",
    "# Pad all sequences to the maximum length found\n",
    "# 'pre' padding means zeros are added at the beginning of shorter sequences\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# In sequence models, we train the model to predict the next word in a sequence. This means:\n",
    "# X (features): The input words the model sees. / y (labels): The next word the model should predict.\n",
    "# Extract input sequences (all words except the last one)\n",
    "x_train_input = input_sequences[:, :-1]  \n",
    "# Extract target labels (the last word in each sequence)\n",
    "y_train_labels = input_sequences[:, -1]  \n",
    "\n",
    "# Convert y labels (word indices) into one-hot encoded vectors\n",
    "# This is necessary because the output layer of the model expects a probability distribution\n",
    "y_train_labels = tf.keras.utils.to_categorical(y_train_labels, num_classes=len(tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Define the LSTM Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding**\n",
    "- The Embedding layer transforms word indices (integers) into dense vector representations.\n",
    "- `input_dim`: Number of unique words (vocabulary size) +1 for padding. Padding are the `0` added to make all sentences the same length. Since we used padding tokens (0), the model must know how to handle 0 as a special token.\n",
    "- `output_dim`: Size of the dense vector that represents each word. The higher the dimension, the more information can be captured about each word.\n",
    "- `input_length`: This is the length of the input sequence to be used to train (longest - 1)\n",
    "\n",
    "**LSTM**\n",
    "- 256 memory units (neurons) that store past information and learn dependencies between words.\n",
    "- Ensures the LSTM outputs the entire sequence, instead of just the last output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxfrischknecht/.pyenv/versions/3.11.5/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "\n",
    "    # Embedding layer: Converts word indices into dense vector representations\n",
    "    # input_dim = vocabulary size (+1 for padding index)\n",
    "    # output_dim = 128 (size of each word vector)\n",
    "    # input_length = max_sequence_length - 1 (because we predict the next word)\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_sequence_length-1),\n",
    "\n",
    "    # First LSTM layer: Processes sequential data\n",
    "    # 256 units = number of memory cells (neurons)\n",
    "    # Maybe 256 is to large is the data set is small > 128\n",
    "    # return_sequences=True allows the next LSTM layer to receive the full sequence output\n",
    "    LSTM(256, return_sequences=True),\n",
    "\n",
    "    # Second LSTM layer: Learns deeper sequence patterns\n",
    "    # 256 units (neurons)\n",
    "    # Since this is the last LSTM layer, return_sequences=False (default)\n",
    "    LSTM(256), # Further reduce complexity\n",
    "\n",
    "    #Dropout(0.3),  # Another Dropout layer\n",
    "\n",
    "    # Fully connected (Dense) output layer:\n",
    "    # Number of neurons = vocabulary size (+1 for padding index)\n",
    "    # Softmax activation converts logits into probability distribution over words\n",
    "    Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function separately\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# Compile the model using the separate loss function\n",
    "# Adam works well for general tasks, but RMSprop is better for sequential tasks like NLP\n",
    "# # Use RMSprop optimizer (better for sequential data)\n",
    "# optimizer=RMSprop(learning_rate=0.0003),\n",
    "# Adam(learning_rate=0.0005)\n",
    "model.compile(\n",
    "            loss=loss_fn, \n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `x_train_input`: The input sequences (X) for training\n",
    "- `y_train_labels`: The corresponding one-hot encoded labels (y)\n",
    "- `epochs=30`: The model will go through the entire dataset 30 times during training. More epochs mean the model learns more, but too many can lead to overfitting.\n",
    "- `batch_size=64`: Use mini-batches of 64 samples for each training step. Instead of training on one sample at a time (slow) or all samples at once (high memory usage), the model is trained in mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on Batch Sizes**\n",
    "\n",
    "🔹 When Does a Bigger Batch Size Help?\n",
    "- If you have a lot of GPU memory → Larger batches train faster.\n",
    "- If you need stable weight updates → Large batches reduce update noise.\n",
    "- For well-defined problems (e.g., image classification) → Large batches work well.\n",
    "\n",
    "🔹 When Should You Use a Smaller Batch Size?\n",
    "- For NLP models (like LSTMs, Transformers) → Small batches generalize better.\n",
    "- If you have limited memory → Small batches fit in memory.\n",
    "- If your dataset is small → Small batches prevent overfitting.\n",
    "\n",
    "🔹 What’s the Best Batch Size?\n",
    "- There’s no universal best batch size—it depends on your dataset and hardware.\n",
    "\n",
    "🔹 Common recommendations:\n",
    "- 32 or 64 → Good for most NLP models.\n",
    "- 128 or 256 → Good for CNNs on large datasets.\n",
    "- 512+ → Only useful if you have a huge dataset & powerful GPU.\n",
    "\n",
    "🔹 Try different values and compare accuracy/loss.\n",
    "- You can use a validation set to find the best batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Metric**            | **Ideal Value**      | **Your Value**  | **Fix Needed?**      |\n",
    "|----------------------|--------------------|----------------|--------------------|\n",
    "| **Training Accuracy**  | ✅ High (~95-98%)   | **97.22%**     | ❌ No issue         |\n",
    "| **Validation Accuracy**| ✅ Similar to training (~90%) | **17.41%** | ✅ **Yes (overfitting)** |\n",
    "| **Training Loss**     | ✅ Low (~0.1 or less) | **0.0729**     | ❌ No issue         |\n",
    "| **Validation Loss**   | ✅ Low (~close to training) | **4.9760**  | ✅ **Yes (big gap)** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 226ms/step - accuracy: 0.0485 - loss: 5.6511\n",
      "Epoch 2/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 236ms/step - accuracy: 0.0605 - loss: 5.5451\n",
      "Epoch 3/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 245ms/step - accuracy: 0.0528 - loss: 5.6249\n",
      "Epoch 4/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 227ms/step - accuracy: 0.0593 - loss: 5.5523\n",
      "Epoch 5/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 211ms/step - accuracy: 0.0553 - loss: 5.4639\n",
      "Epoch 6/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 244ms/step - accuracy: 0.0653 - loss: 5.3312\n",
      "Epoch 7/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 258ms/step - accuracy: 0.0647 - loss: 5.2943\n",
      "Epoch 8/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 231ms/step - accuracy: 0.0767 - loss: 5.1514\n",
      "Epoch 9/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 224ms/step - accuracy: 0.0913 - loss: 5.0058\n",
      "Epoch 10/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 209ms/step - accuracy: 0.1127 - loss: 4.8481\n",
      "Epoch 11/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 232ms/step - accuracy: 0.1234 - loss: 4.6608\n",
      "Epoch 12/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 224ms/step - accuracy: 0.1300 - loss: 4.4585\n",
      "Epoch 13/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 236ms/step - accuracy: 0.1652 - loss: 4.2332\n",
      "Epoch 14/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 227ms/step - accuracy: 0.1469 - loss: 4.1462\n",
      "Epoch 15/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 219ms/step - accuracy: 0.1851 - loss: 3.9054\n",
      "Epoch 16/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 229ms/step - accuracy: 0.2085 - loss: 3.7728\n",
      "Epoch 17/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 230ms/step - accuracy: 0.2325 - loss: 3.6174\n",
      "Epoch 18/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 244ms/step - accuracy: 0.2331 - loss: 3.4760\n",
      "Epoch 19/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 226ms/step - accuracy: 0.2596 - loss: 3.3109\n",
      "Epoch 20/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 241ms/step - accuracy: 0.3040 - loss: 3.1287\n",
      "Epoch 21/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 231ms/step - accuracy: 0.3379 - loss: 2.9242\n",
      "Epoch 22/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 215ms/step - accuracy: 0.3688 - loss: 2.8403\n",
      "Epoch 23/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 225ms/step - accuracy: 0.4341 - loss: 2.6170\n",
      "Epoch 24/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 246ms/step - accuracy: 0.4702 - loss: 2.4547\n",
      "Epoch 25/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 219ms/step - accuracy: 0.5200 - loss: 2.3727\n",
      "Epoch 26/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 228ms/step - accuracy: 0.5455 - loss: 2.2210\n",
      "Epoch 27/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 242ms/step - accuracy: 0.5944 - loss: 2.0680\n",
      "Epoch 28/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 241ms/step - accuracy: 0.6393 - loss: 1.9132\n",
      "Epoch 29/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 227ms/step - accuracy: 0.6651 - loss: 1.8323\n",
      "Epoch 30/30\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 228ms/step - accuracy: 0.6996 - loss: 1.7250\n"
     ]
    }
   ],
   "source": [
    "# use early stopping to prevent overfitting when training with more epochs\n",
    "# If val_loss does not improve for patience=5 consecutive epochs, training stops.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Since NLP models often work better with larger batches: Larger batch size stabilizes updates and avoids too much noise.\n",
    "history = model.fit(x_train_input, y_train_labels, epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved!\n"
     ]
    }
   ],
   "source": [
    "# save the model manually for later contiuing\n",
    "# model.save(\"anime_quote_generator_manual_checkpoint.h5\")\n",
    "model.save(\"anime_quote_generator.keras\")\n",
    "print(\"Checkpoint saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize overfitting by plotting the accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract training history\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    }
   ],
   "source": [
    "# Extract training history\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs. Validation Accuracy')\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training vs. Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate New Quotes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Generates a new quote by predicting the next words based on seed_text.\n",
    "\n",
    "Parameters:\n",
    "- seed_text (str): The initial text to start generating from.\n",
    "- next_words (int): Number of words to generate.\n",
    "\n",
    "Returns: \n",
    "- str: The generated quote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Generate New Quotes\n",
    "def generate_quote(seed_text, next_words=20):\n",
    "    # Loop to generate 'next_words' number of words\n",
    "    for _ in range(next_words):\n",
    "        \n",
    "        # Convert the input text into a sequence of token indices\n",
    "        tokenized = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        \n",
    "        # Pad the sequence to match the expected input size of the model\n",
    "        # Padding ensures the sequence is of the same length as used during training\n",
    "        tokenized = pad_sequences([tokenized], maxlen=max_sequence_length-1, padding='pre')\n",
    "        \n",
    "        # Predict the next word using the trained model\n",
    "        # np.argmax() gets the index of the highest probability word\n",
    "        predicted = np.argmax(model.predict(tokenized, verbose=0), axis=-1)\n",
    "        \n",
    "        # Convert the predicted word index back to an actual word\n",
    "        # If the word is not found, return an empty string to avoid errors\n",
    "        word = tokenizer.index_word.get(predicted[0], '')\n",
    "\n",
    "        # Append the predicted word to the seed text\n",
    "        seed_text += \" \" + word\n",
    "\n",
    "    # Return the final generated quote\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a random new quote**\n",
    "- Select a random seed quote to give the model context to start from. \n",
    "- This is not necessary but encouraged as the model might make unpredictable things\n",
    "    1.\tIf the words in seed_text exist in the vocabulary, the model treats it like any other input.\n",
    "\t2.\tIf the words are not in the vocabulary, they will be ignored or tokenized as Unknown (UNK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Quote:\n",
      "I’ll leave tomorrow’s problems to tomorrow’s me. \n",
      "\n",
      "Generated Quote:\n",
      "I’ll leave tomorrow’s problems to tomorrow’s me. them to do anything always something for i have the\n"
     ]
    }
   ],
   "source": [
    "random_quote = random.choice(quotes[:50])\n",
    "# start_seed = random_quote\n",
    "print(\"Selected Quote:\")\n",
    "print(f\"{random_quote} \\n\")\n",
    "\n",
    "print(\"Generated Quote:\")\n",
    "print(generate_quote(random_quote, next_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
